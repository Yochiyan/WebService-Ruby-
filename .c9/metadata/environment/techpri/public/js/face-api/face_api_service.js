{"changed":true,"filter":false,"title":"face_api_service.js","tooltip":"/techpri/public/js/face-api/face_api_service.js","value":"// ここで対応するHTMLに書かれているパーツを、パーツのidから探して取得しています。\nconst video = document.getElementById(\"video\");\nvar stream;\nvar creatingImage = true;\n\nvar emotion = document.getElementById(\"emotion\").className;\n\n// もし加工の種類を増やしたのなら、HTMLのcanvasを増やし、この配列を長くすることで処理が動くようになります。\n// 画像を用いた加工は、メンターさんに聞きながら自分で増やしてくださいね。\nvar canvasArray = [\n  document.getElementById(\"canvas1\"),\n  document.getElementById(\"canvas2\"),\n  document.getElementById(\"canvas3\"),\n];\n\nvar message = document.getElementById(\"message\");\nvar emotionValue = document.getElementById(\"emotionValue\");\n\nconst inputFile1 = document.getElementById(\"photo1\");\nconst inputFile2 = document.getElementById(\"photo2\");\nconst inputFile3 = document.getElementById(\"photo3\");\n\nconst form = document.getElementById(\"takePhotoForm\");\n\n// フォントを読み込んでいます。\nconst fontFace = new FontFace(\n  \"mini-wakuwaku-maru\",\n  \"url(/font/mini-wakuwaku-maru.otf)\"\n);\n\nwindow.onload = function () {\n  // 顔や表情を検出するface-apiを読み込んでいます。\n  Promise.all([\n    faceapi.loadSsdMobilenetv1Model(\"/js/face-api/weights\"),\n    faceapi.loadFaceLandmarkModel(\"/js/face-api/weights\"),\n    faceapi.loadFaceRecognitionModel(\"/js/face-api/weights\"),\n    faceapi.loadFaceExpressionModel(\"/js/face-api/weights\"),\n  ]).then(() => {\n    console.log(\"finished reading faceAPI\");\n  });\n\n  // カメラにアクセスし、リアルタイムに画像を表示する設定をします。\n  navigator.mediaDevices\n    .getUserMedia({\n      video: true,\n      audio: false,\n    })\n    .then(async function (stream) {\n      stream = stream;\n      if (video) {\n        video.srcObject = stream;\n        await video.play();\n      }\n      // 読み込まれたすぐ後に、カメラ画面をフェードインさせます。\n      setTimeout(function () {\n        cameraViewFadeIn();\n      }, 100);\n    })\n    .catch((e) => {\n      console.log(e);\n    });\n\n  // フォントを読み込み始めます。\n  fontFace.load();\n\n  // 検出する表情がサーバーで設定されていないなら、えがおを設定します。\n  if (emotion == \"\" || emotion == null) {\n    emotion = \"happy\";\n  }\n};\n\n// これが呼び出されるとformが送信されます。\n// 逆に、これが呼び出されるまでformが送信されないような仕組みになっています。\nfunction submitForm() {\n  creatingImage = false;\n  form.submit();\n}\n\n// script.jsで呼び出している関数です。さつえいボタンが押された後の処理が書かれています。\nfunction takePhoto() {\n  // カウントダウンを表示します。\n  countdown();\n\n  // カウントダウンが終わる3秒後に、videoに表示されている画像をcanvasに映し取っています。\n  setTimeout(function () {\n    var contextCanvas1 = canvasArray[0].getContext(\"2d\");\n    var contextCanvas2 = canvasArray[1].getContext(\"2d\");\n    var contextCanvas3 = canvasArray[2].getContext(\"2d\");\n    var w = video.clientWidth;\n    var h = video.clientHeight;\n    canvasArray[0].setAttribute(\"width\", w);\n    canvasArray[0].setAttribute(\"height\", h);\n    canvasArray[1].setAttribute(\"width\", w);\n    canvasArray[1].setAttribute(\"height\", h);\n    canvasArray[2].setAttribute(\"width\", w);\n    canvasArray[2].setAttribute(\"height\", h);\n\n    contextCanvas1.translate(w, 0);\n    contextCanvas1.scale(-1, 1);\n    contextCanvas2.translate(w, 0);\n    contextCanvas2.scale(-1, 1);\n    contextCanvas3.translate(w, 0);\n    contextCanvas3.scale(-1, 1);\n\n    contextCanvas1.drawImage(video, 0, 0, w, h);\n    contextCanvas2.drawImage(video, 0, 0, w, h);\n    contextCanvas3.drawImage(video, 0, 0, w, h);\n\n    contextCanvas1.translate(w, 0);\n    contextCanvas1.scale(-1, 1);\n    contextCanvas2.translate(w, 0);\n    contextCanvas2.scale(-1, 1);\n    contextCanvas3.translate(w, 0);\n    contextCanvas3.scale(-1, 1);\n\n    // face-apiを用いて顔や表情を検出します。\n    detectFace();\n  }, 3000);\n}\n\n// カウントダウンのアニメーションを表示します。\nfunction countdown() {\n  var timer = document.getElementById(\"timer\");\n  var timerBackground = document.getElementById(\"timer-background\");\n\n  // timerとその背景を表示します。\n  timer.style.display = \"inline-block\";\n  timerBackground.style.display = \"flex\";\n\n  // 秒数をずらしながらtimerのテキストを変更しています。\n  timer.innerText = \"3\";\n  timer.animate([{ opacity: \"1\" }, { opacity: \"0\" }], { duration: 1500 });\n\n  setTimeout(function () {\n    timer.innerText = \"2\";\n    timer.animate([{ opacity: \"1\" }, { opacity: \"0\" }], { duration: 1500 });\n  }, 1000);\n\n  setTimeout(function () {\n    timer.innerText = \"1\";\n    timer.animate([{ opacity: \"1\" }, { opacity: \"0\" }], { duration: 1500 });\n  }, 2000);\n\n  setTimeout(function () {\n    timer.innerText = \"処理中…\";\n    timer.style.opacity = 1;\n  }, 3000);\n}\n\n// face-apiを用いて顔や表情を検出します。また、ここで画像の加工も行なっています。\nasync function detectFace() {\n  // この時点ではcanvasArrayの中に入っているcanvasは全て同じ画像なので、一番最初の要素を代表として利用し、face-apiに読み込ませています。\n  const faceData = await faceapi\n    .detectAllFaces(canvasArray[0])\n    .withFaceLandmarks()\n    .withFaceExpressions();\n\n  // faceDataには検出された顔のデータが人数分入っています。\n  // これの長さが0だったら検出された顔がないということなので、no_face画面に行きます。\n  if (!faceData.length) {\n    noFace();\n    return;\n  }\n\n  // 感情の判定に用いる数値は、全員の感情の平均値としています。\n  // もしこれを、全員の中の最大値や最小値にしたい場合、下に用意してある「maxEmotionOf」、「minEmotionOf」を用いてくださいね。\n  let _emotionValue = averageEmotionOf(faceData);\n\n  // ここでは0.2という値を境界にしています。製作者の勘ですので、変えてみるのも面白いかもしれませんね。\n  if (_emotionValue < 0.2) {\n    lessEmotion(_emotionValue);\n    return;\n  }\n\n  // ここまでくれば、顔も検出されて感情も足りているので、結果はsuccessとなります。これをmessageに付与し、formを通じてサーバーに送信しています。\n  message.value = \"success\";\n\n  // canvasのcontextを配列にまとめます。\n  const contextArray = [\n    canvasArray[0].getContext(\"2d\"),\n    canvasArray[1].getContext(\"2d\"),\n    canvasArray[2].getContext(\"2d\"),\n  ];\n\n  // サーバーから送られてきた画像をここに持たせています。\n  // JavaScriptに画像を読み込む方法はいくつかありますが、こうするとHTMLから簡単に画像を変更することができます。\n  const material = document.getElementById(\"material\");\n\n  // ここでは上と別の方法で画像を取得しています。\n  const cheekImage = await loadImage(\"/img/materials/hoppe2.png\");\n  const nekomimiLeftImage = await loadImage(\"/img/materials/mimi_left.png\");\n  const nekomimiRightImage = await loadImage(\"/img/materials/mimi_right.png\");\n  const nekohanaImage = await loadImage(\"/img/materials/hana.png\");\n\n  // 画像を白くするために使う値です。これを大きくするとより白くなります。\n  const gamma = 1.5;\n\n  contextArray.forEach(function (context) {\n    // contextに文字を記入する際のフォントを設定します。\n    context.font = \"25px mini-wakuwaku-maru\";\n    context.textAlign = \"center\";\n    context.strokeStyle = \"#FFF\";\n    context.lineWidth = 7;\n    context.fillStyle = \"#FF53B0\";\n\n    // 画像をgamma値を用いて白くします。\n    var src = context.getImageData(0, 0, canvasArray[0].width, canvasArray[0].height);\n    var dst = context.createImageData(canvasArray[0].width, canvasArray[0].height);\n\n    for (var i = 0; i < src.data.length; i = i + 4) {\n      dst.data[i] = ~~(255 * Math.pow(src.data[i] / 255, 1 / gamma));\n      dst.data[i + 1] = ~~(255 * Math.pow(src.data[i + 1] / 255, 1 / gamma));\n      dst.data[i + 2] = ~~(255 * Math.pow(src.data[i + 2] / 255, 1 / gamma));\n      dst.data[i + 3] = src.data[i + 3];\n    }\n\n    // 変更したデータをcontextに反映します。\n    context.putImageData(dst, 0, 0);\n  });\n\n  // 検出された顔それぞれに対して加工をしていきます。\n  for (let i = 0; i < faceData.length; i++) {\n    // 顔の特徴的な点の座標を取得します。\n    const chin = faceData[i].landmarks.positions[8];\n    const leftCheek = faceData[i].landmarks.positions[1];\n    const rightCheek = faceData[i].landmarks.positions[15];\n    const noseTop = faceData[i].landmarks.positions[30];\n    const leftEye = faceData[i].landmarks.positions[36];\n    const rightEye = faceData[i].landmarks.positions[45];\n    const leftEyebrow = faceData[i].landmarks.positions[19];\n    const rightEyebrow = faceData[i].landmarks.positions[24];\n\n    // 顔の横幅を計算しています。\n    const faceWidth = Math.sqrt(Math.pow(rightCheek.x - leftCheek.x, 2) + Math.pow(rightCheek.y - leftCheek.y, 2));\n    // 顔の傾きを計算しています。\n    const faceLean = Math.atan2(rightEye.y - leftEye.y, rightEye.x - leftEye.x);\n\n    // 文字を書き込みます。\n    contextArray.forEach(function (context) {\n      context.save();\n\n      // あご先に移動し、顔の傾きだけ座標を傾けます。\n      context.translate(chin.x, chin.y);\n      context.rotate(faceLean);\n\n      // 2回に分けて文字を書き込み、ふちどりを追加しています。\n      context.strokeText(emotionText(faceData[i]), 0, 35);\n      context.fillText(emotionText(faceData[i]), 0, 35);\n\n      context.restore();\n    });\n\n    // 1つ目のcanvasに対し��加工を行います。\n    var context = contextArray[0];\n    // materialから取得した画像を右頬に貼り付けます。\n    const iconSize = faceWidth * 0.3;\n    context.save();\n\n    context.translate(rightCheek.x, rightCheek.y);\n    context.rotate(faceLean);\n\n    // 移動した座標を元に、画像を右頬に貼り付けます。\n    context.drawImage(material, -iconSize / 2 - 10, -5, iconSize, iconSize);\n\n    context.restore();\n\n    // 2つ目のcanvasに対して加工を行います。\n    context = contextArray[1];\n    // 両頬にcheekImageを貼り付けます。\n    const cheekImageWidth = faceWidth * 0.4;\n    const cheekImageHeight = faceWidth * 0.3;\n    context.save();\n\n    context.translate(leftCheek.x, leftCheek.y);\n    context.rotate(faceLean);\n\n    context.drawImage(cheekImage, -10, -5, cheekImageWidth, cheekImageHeight);\n\n    context.translate(rightCheek.x - leftCheek.x, 0);\n\n    context.drawImage(cheekImage, -cheekImageWidth + 10, -5, cheekImageWidth, cheekImageHeight);\n\n    context.restore();\n\n    // 3つ目のcanvasに対して加工を行います。\n    context = contextArray[2];\n    // ねこみみ、ねこはなを貼り付けます。\n    const nekomimi_size = faceWidth * 0.33;\n    const nekohana_size = faceWidth;\n    context.save();\n\n    context.translate(leftEyebrow.x, leftEyebrow.y);\n    context.rotate(faceLean);\n\n    context.drawImage(nekomimiLeftImage, -(nekomimi_size), -70, nekomimi_size, nekomimi_size);\n\n    context.translate(rightEyebrow.x - leftEyebrow.x, 0);\n\n    context.drawImage(nekomimiRightImage, 0, -70, nekomimi_size, nekomimi_size);\n\n    context.restore();\n\n    context.save();\n\n    context.translate(noseTop.x, noseTop.y);\n    context.rotate(faceLean);\n\n    context.drawImage(nekohanaImage, -(nekohana_size / 2), -20, nekohana_size, 40);\n\n    context.restore();\n  }\n\n  // formに加工した画像のデータを受け渡します。\n  inputFile1.value = canvasArray[0].toDataURL(\"image/jpeg\");\n  inputFile2.value = canvasArray[1].toDataURL(\"image/jpeg\");\n  inputFile3.value = canvasArray[2].toDataURL(\"image/jpeg\");\n\n  // 全ての処理が終わってから、formを動かします。\n  submitForm();\n}\n\n// 顔が検出されなかった場合の処理です。\nfunction noFace() {\n  // messageをno faceにして、サーバーに顔が検出されなかったことを知らせます。\n  message.value = \"no face\";\n\n  submitForm();\n}\n\n// 感情が足りなかった場合の処理です。\nfunction lessEmotion(value) {\n  // messageをless emotionにして、サーバーに感情が足りなかったことを知らせます。\n  message.value = \"less emotion\";\n  // 表示する感情の値は、四捨五入で整数にしています。\n  emotionValue.value = Math.round(value_convert(value) * 100);\n\n  submitForm();\n}\n\n// 全員の感情の平均値を計算します。\nfunction averageEmotionOf(faceData) {\n  var average = 0;\n\n  for (let i = 0; i < faceData.length; i++) {\n    average += value_convert(faceData[i][\"expressions\"][emotion]) / faceData.length;\n  }\n\n  return average;\n}\n\n// 全員の感情の中で最小の値を計算します。\nfunction minEmotionOf(faceData) {\n  return Math.min(...faceData.map((data) => value_convert(data[\"expressions\"][emotion])));\n}\n\n// 全員の感情の中で最大の値を計算します。\nfunction maxEmotionOf(faceData) {\n  return Math.max(...faceData.map((data) => value_convert(data[\"expressions\"][emotion])));\n}\n\n// face-apiから得られた感情の数値をいい感じに調節します。\nfunction value_convert(x) {\n  return Math.max(x / 2, 10 * x - 9.2, 3000 * x - 2999);\n}\n\n// 指定された感情に対し、あご先に貼り付ける文章中の感情の名前を提供します。\nfunction expressionName() {\n  switch (emotion) {\n    case \"angry\":\n      return \"いかり\";\n    case \"disgusted\":\n      return \"うんざり\";\n    case \"fearful\":\n      return \"きょうふ\";\n    case \"happy\":\n      return \"えがお\";\n    case \"neutral\":\n      return \"まがお\";\n    case \"sad\":\n      return \"かなしみ\";\n    case \"surprised\":\n      return \"おどろき\";\n    default:\n      return \"\";\n  }\n}\n\n// あご先に貼り付ける文章を作ります。\n// こっちでは数値を切り捨てにしています。めざせ100％！\nfunction emotionText(faceData) {\n  const emotionValue = Math.floor(value_convert(faceData[\"expressions\"][emotion]) * 100);\n\n  // ${}の中に変数や関数を書くことで、その値を文字列中に取り込むことができます。\n  return `${expressionName()} ${emotionValue}%`;\n}\n\n// 画像を読み込むための関数です。\nasync function loadImage(imgUrl) {\n  var img = null;\n  var promise = new Promise(function (resolve) {\n    img = new Image();\n    img.onload = function () {\n      resolve();\n    };\n    img.src = imgUrl;\n  });\n  await promise;\n  return img;\n}\n","undoManager":{"mark":-2,"position":-1,"stack":[]},"ace":{"folds":[],"scrolltop":6048.699999999992,"scrollleft":0,"selection":{"start":{"row":397,"column":34},"end":{"row":397,"column":34},"isBackwards":false},"options":{"guessTabSize":true,"useWrapMode":false,"wrapToView":true},"firstLineState":{"row":377,"state":"start","mode":"ace/mode/javascript"}},"timestamp":1691890278999}